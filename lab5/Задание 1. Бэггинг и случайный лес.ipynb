{"cells":[{"cell_type":"markdown","metadata":{"id":"CkLlg2KZha_T"},"source":["# Бэггинг и случайный лес"]},{"cell_type":"markdown","metadata":{"id":"8w-YE_zRha_W"},"source":["Загрузите датасет digits с помощью функции load_digits из sklearn.datasets и подготовьте матрицу признаков X и ответы на обучающей выборке y (вам потребуются поля data и target в объекте, который возвращает load_digits).\n","Для оценки качества далее нужно будет использовать cross_val_score из sklearn.cross_validation с параметром cv=10. Эта функция реализует k-fold cross validation c k равным значению параметра cv. Функция cross_val_score будет возвращать numpy.ndarray, в котором будет k чисел - качество в каждом из k экспериментов k-fold cross validation. Для получения среднего значения (которое и будет оценкой качества работы) вызовите метод .mean() у массива, который возвращает cross_val_score.\n","\n","Если вам захочется ускорить вычисление cross_val_score - можете попробовать использовать параметр n_jobs."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"MlAw4VdSha_X"},"outputs":[],"source":["%matplotlib inline\n","from sklearn import datasets, cross_validation\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import seaborn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHIPozbWha_a"},"outputs":[],"source":["# your code\n","print dataset['DESCR']"]},{"cell_type":"markdown","metadata":{"id":"w4LcPJ0Jha_b"},"source":["### Шаг 1\n","Создайте DecisionTreeClassifier с настройками по умолчанию и измерьте качество его работы с помощью cross_val_score."]},{"cell_type":"code","source":[],"metadata":{"id":"mb1huconh6gk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVBa5JdTha_c"},"source":["### Шаг 2\n","\n","Воспользуйтесь BaggingClassifier из sklearn.ensemble, чтобы обучить бэггинг над DecisionTreeClassifier. Используйте в BaggingClassifier параметры по умолчанию, задав только количество деревьев равным 100."]},{"cell_type":"code","source":[],"metadata":{"id":"Y41aypEEh71g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RHFYUwcKha_f"},"source":["### Шаг 3\n","Теперь изучите параметры BaggingClassifier и выберите их такими, чтобы каждый базовый алгоритм обучался не на всех d признаках, а на $\\sqrt{d}$ случайных признаков. Корень из числа признаков - часто используемая эвристика в задачах классификации, в задачах регрессии же часто берут число признаков, деленное на три. Но в общем случае ничто не мешает вам выбирать любое другое число случайных признаков."]},{"cell_type":"code","source":[],"metadata":{"id":"vkaDwV1MiAa8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9jrDgt6-ha_g"},"source":["### Шаг 4\n","Наконец, давайте попробуем выбирать случайные признаки не один раз на все дерево, а при построении каждой вершины дерева. Сделать это несложно: нужно убрать выбор случайного подмножества признаков в BaggingClassifier и добавить его в DecisionTreeClassifier. Какой параметр за это отвечает, можно понять из документации sklearn, либо просто попробовать угадать (скорее всего, у вас сразу получится). Попробуйте выбирать опять же $\\sqrt{d}$ признаков."]},{"cell_type":"code","source":[],"metadata":{"id":"RL9SE6HMiJs_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fzt8O5Zjha_g"},"source":["### Шаг 5\n","\n","Полученный в пункте 4 классификатор - бэггинг на рандомизированных деревьях (в которых при построении каждой вершины выбирается случайное подмножество признаков и разбиение ищется только по ним). Это в точности соответствует алгоритму Random Forest, поэтому почему бы не сравнить качество работы классификатора с RandomForestClassifier из sklearn.ensemble. Сделайте это, а затем изучите, как качество классификации на данном датасете зависит от количества деревьев, количества признаков, выбираемых при построении каждой вершины дерева, а также ограничений на глубину дерева. Для наглядности постройте графики зависимости качества от значений параметров.\n","\n","На основе наблюдений выпишите через пробел номера правильных утверждений из приведенных ниже в порядке возрастания номера\n","\n","  1. Случайный лес сильно переобучается с ростом количества деревьев\n","  2. При очень маленьком числе деревьев (5, 10, 15), случайный лес работает хуже, чем при большем числе деревьев\n","  3. С ростом количества деревьев в случайном лесе, в какой-то момент деревьев становится достаточно для высокого качества классификации, а затем качество существенно не меняется.\n","  4. При большом количестве признаков (для данного датасета - 40, 50) качество классификации становится хуже, чем при малом количестве признаков (5, 10). Это связано с тем, что чем меньше признаков выбирается в каждом узле, тем более различными получаются деревья (ведь деревья сильно неустойчивы к изменениям в обучающей выборке), и тем лучше работает их композиция.\n","  5. При большом количестве признаков (40, 50, 60) качество классификации лучше, чем при малом количестве признаков (5, 10). Это связано с тем, что чем больше признаков - тем больше информации об объектах, а значит алгоритм может делать прогнозы более точно.\n","  6. При небольшой максимальной глубине деревьев (5-6) качество работы случайного леса намного лучше, чем без ограничения глубины, т.к. деревья получаются не переобученными. С ростом глубины деревьев качество ухудшается.\n","  7. При небольшой максимальной глубине деревьев (5-6) качество работы случайного леса заметно хуже, чем без ограничений, т.к. деревья получаются недообученными. С ростом глубины качество сначала улучшается, а затем не меняется существенно, т.к. из-за усреднения прогнозов и различий деревьев их переобученность в бэггинге не сказывается на итоговом качестве (все деревья преобучены по-разному, и при усреднении они компенсируют переобученность друг-друга)."]},{"cell_type":"code","source":[],"metadata":{"id":"iksN9G7hicTv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6EwstwOha_h"},"source":["#### Зависимость от количества деревьев"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJEFJzwwha_h"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mEp4itOCha_i"},"source":["#### Зависимость от количества признаков"]},{"cell_type":"code","source":[],"metadata":{"id":"My_vsyyiifPl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_DuRP3bzha_j"},"source":["#### Зависимость от глубины деревьев"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"6U-mgaFUha_j"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}